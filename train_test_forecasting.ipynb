{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e520362-af32-48d5-94ec-d41cd0019a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting einops\n",
      "  Downloading einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "Installing collected packages: einops\n",
      "Successfully installed einops-0.8.1\n"
     ]
    }
   ],
   "source": [
    "!pip install einops\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import seaborn as sns\n",
    "from dataclasses import dataclass\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import datautils\n",
    "from utils import init_dl_program\n",
    "from utils import find_closest_train_segment\n",
    "from hdst import HDST\n",
    "import torch\n",
    "import gc\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f05ed6-5654-439d-b90c-f536b47e49b0",
   "metadata": {},
   "source": [
    "## Create Args Configuration Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af8d59d2-5f8f-4473-a0d7-eddf3b4943f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Args:\n",
    "    task_weights: dict\n",
    "    dataset: str = \"\"\n",
    "    loader: str = \"\"\n",
    "    gpu: int = 0\n",
    "    static_repr_dims: int = 128\n",
    "    dynamic_repr_dims: int = 128\n",
    "    epochs: int = 200\n",
    "\n",
    "    run_name: str = \"\"\n",
    "    batch_size: int = 16\n",
    "    lr: float = 0.001\n",
    "    max_train_length = 800\n",
    "    iters: int = None\n",
    "    save_every = None\n",
    "    seed: int = 1234\n",
    "    max_threads = None\n",
    "    eval: bool = True\n",
    "    irregular = 0\n",
    "\n",
    "    sample_size: int = 50\n",
    "    window_size: int = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bc736c-ca43-4a45-b7b8-d5297b6d74ca",
   "metadata": {},
   "source": [
    "## Instantiate Args Configuration Class\n",
    "Create an instance of arguments, specifying the necessary arguments and those important to your use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51964981-9e2a-4dfa-bbc8-badb1e2ec44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args(\n",
    "    static_repr_dims=128,\n",
    "    dynamic_repr_dims=128,\n",
    "    task_weights={\n",
    "        'local_static_contrast': 0.33,\n",
    "        'global_vatiant_contrast': 0.33,\n",
    "        'dynamic_trend_pred': 0.34,\n",
    "    },\n",
    "    eval=False,\n",
    "    batch_size=32,\n",
    ")\n",
    "\n",
    "device = init_dl_program(args.gpu, seed=args.seed, max_threads=args.max_threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2757d0-70eb-4625-b386-cf7d380fd4cd",
   "metadata": {},
   "source": [
    "## Load your data\n",
    "You can use any data, as long as it is an `np.ndarray` of shape $(N, T, C)$ where $N$ is the number of time-series instances, $T$ the number of timesteps per instance, and $C$ the number of channels.\n",
    "Choosing from 'ETTh1', 'ETTh2', 'ETTm1' and 'ETTm2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8966d2-184d-4ae8-a81a-caa095e78025",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, train_slice, valid_slice, test_slice, scaler, pred_lens = datautils.load_forecast_csv(\"ETTh1\")\n",
    "train_data = data[:, train_slice]\n",
    "test_data = data[:, test_slice]\n",
    "print(f\"Shapes - train data: {train_data.shape}, test data: {test_data.shape}\")\n",
    "print(pred_lens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc89e316-5120-4e3d-919e-83ab4b27a4dc",
   "metadata": {},
   "source": [
    "## Create and Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e0a080c-c734-4bb7-a33b-13e33253b61a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sns.set_theme()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce8c7e4-9a28-40fc-99b7-abc2abc9608f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (15, 640, 2)\n",
      "Epoch #0: loss=2.6060452461242676\n",
      "Epoch #1: loss=2.790367603302002\n",
      "Epoch #2: loss=2.5673162937164307\n",
      "Epoch #3: loss=2.579359769821167\n",
      "Epoch #4: loss=2.565676212310791\n",
      "Epoch #5: loss=2.547593116760254\n",
      "Epoch #6: loss=2.523077964782715\n",
      "Epoch #7: loss=2.544933319091797\n",
      "Epoch #8: loss=2.5110695362091064\n",
      "Epoch #9: loss=2.489795446395874\n",
      "Epoch #10: loss=2.4702415466308594\n",
      "Epoch #11: loss=2.459918975830078\n",
      "Epoch #12: loss=2.4439103603363037\n",
      "Epoch #13: loss=2.4257798194885254\n",
      "Epoch #14: loss=2.4186747074127197\n",
      "Epoch #15: loss=2.4045932292938232\n",
      "Epoch #16: loss=2.399014472961426\n",
      "Epoch #17: loss=2.3740293979644775\n",
      "Epoch #18: loss=2.3643035888671875\n",
      "Epoch #19: loss=2.3630001544952393\n",
      "Epoch #20: loss=2.347736120223999\n",
      "Epoch #21: loss=2.353487730026245\n",
      "Epoch #22: loss=2.34149169921875\n",
      "Epoch #23: loss=2.342567205429077\n",
      "Epoch #24: loss=2.3189587593078613\n",
      "Epoch #25: loss=2.324443817138672\n",
      "Epoch #26: loss=2.3308908939361572\n",
      "Epoch #27: loss=2.2996582984924316\n",
      "Epoch #28: loss=2.3004984855651855\n",
      "Epoch #29: loss=2.3030450344085693\n",
      "Epoch #30: loss=2.289593458175659\n",
      "Epoch #31: loss=2.2773780822753906\n",
      "Epoch #32: loss=2.277212381362915\n",
      "Epoch #33: loss=2.2754266262054443\n",
      "Epoch #34: loss=2.2561168670654297\n",
      "Epoch #35: loss=2.240994930267334\n",
      "Epoch #36: loss=2.258650064468384\n",
      "Epoch #37: loss=2.2469117641448975\n",
      "Epoch #38: loss=2.2419207096099854\n",
      "Epoch #39: loss=2.2449324131011963\n",
      "Epoch #40: loss=2.2080225944519043\n",
      "Epoch #41: loss=2.2288525104522705\n",
      "Epoch #42: loss=2.2081239223480225\n",
      "Epoch #43: loss=2.212700366973877\n",
      "Epoch #44: loss=2.205080032348633\n",
      "Epoch #45: loss=2.2061712741851807\n",
      "Epoch #46: loss=2.196614980697632\n",
      "Epoch #47: loss=2.1886508464813232\n",
      "Epoch #48: loss=2.1822333335876465\n",
      "Epoch #49: loss=2.2041406631469727\n",
      "Epoch #50: loss=2.1726391315460205\n",
      "Epoch #51: loss=2.17405104637146\n",
      "Epoch #52: loss=2.171264886856079\n",
      "Epoch #53: loss=2.155846118927002\n",
      "Epoch #54: loss=2.1753532886505127\n",
      "Epoch #55: loss=2.151139736175537\n",
      "Epoch #56: loss=2.1558587551116943\n",
      "Epoch #57: loss=2.15630841255188\n",
      "Epoch #58: loss=2.1379663944244385\n",
      "Epoch #59: loss=2.141061782836914\n",
      "Epoch #60: loss=2.1185142993927\n",
      "Epoch #61: loss=2.1354610919952393\n",
      "Epoch #62: loss=2.1221704483032227\n",
      "Epoch #63: loss=2.1138498783111572\n",
      "Epoch #64: loss=2.1278891563415527\n",
      "Epoch #65: loss=2.109076738357544\n",
      "Epoch #66: loss=2.1055331230163574\n",
      "Epoch #67: loss=2.0973875522613525\n",
      "Epoch #68: loss=2.0967092514038086\n",
      "Epoch #69: loss=2.100405216217041\n",
      "Epoch #70: loss=2.081296443939209\n",
      "Epoch #71: loss=2.092583179473877\n",
      "Epoch #72: loss=2.091569185256958\n",
      "Epoch #73: loss=2.1023764610290527\n",
      "Epoch #74: loss=2.086991310119629\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "model = HDST(\n",
    "    input_dims=train_data.shape[-1],\n",
    "    device=device,\n",
    "    task_weights=args.task_weights,\n",
    "    batch_size=args.batch_size,\n",
    "    lr=args.lr,\n",
    "    output_dims1=args.static_repr_dims,\n",
    "    output_dims2=args.dynamic_repr_dims,\n",
    "    max_train_length=args.max_train_length\n",
    ")\n",
    "\n",
    "loss_log = model.fit(\n",
    "    train_data,\n",
    "    n_epochs=args.epochs,\n",
    "    n_iters=args.iters,\n",
    "    k=args.sample_size,\n",
    "    w=args.window_size\n",
    ")\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "print(f\"Model training time: {training_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2801942-d5d0-47a1-9f33-f82006f12425",
   "metadata": {},
   "source": [
    "Save model and loss log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee8e10d-aead-4107-ab00-6b34690ca303",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.save(model, 'mymodel.pth')\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({\"epoch\": list(range(1, len(loss_log) + 1)), \"loss\": loss_log})\n",
    "df.to_csv(\"loss_log.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b646a4b-0291-49df-a19b-aa40b4549e5d",
   "metadata": {},
   "source": [
    "Visualize the loss curve and save figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a97b47a-04d8-45e0-969e-f74ac670576d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# print(loss_log)\n",
    "plt.plot(loss_log, label=\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss Curve\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(\"loss_curve.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc062a5-48c4-4671-b78d-0dc8d4420c77",
   "metadata": {},
   "source": [
    "## Test the model\n",
    "Firstly, get the representation of training data and test data using `model.encode`. Then train a Ridge regression model using training representation and corresponding training labels. Finally, use the Ridge regression model to do forecasting for test representation. Report the MSE and MAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bba3a8-aa7a-4566-8a39-01abb8bbd31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tasks.forecasting import eval_forecasting\n",
    "out, eval_res = eval_forecasting(\n",
    "                model,\n",
    "                data,\n",
    "                train_slice,\n",
    "                valid_slice,\n",
    "                test_slice,\n",
    "                scaler,\n",
    "                pred_lens\n",
    "            )\n",
    "res=eval_res['ours']\n",
    "for key, value in res.items():\n",
    "    print(f\"{key}: {value}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
