{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting einops\n",
      "  Downloading einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "Installing collected packages: einops\n",
      "Successfully installed einops-0.8.1\n"
     ]
    }
   ],
   "source": [
    "!pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 59,
     "status": "ok",
     "timestamp": 1742153356160,
     "user": {
      "displayName": "Jia Wang",
      "userId": "17769915354902194661"
     },
     "user_tz": 240
    },
    "id": "X9HkfBz93xPF",
    "outputId": "75995636-bd06-4294-f6ae-a179f269fa25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import seaborn as sns\n",
    "from dataclasses import dataclass\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import datautils\n",
    "from utils import init_dl_program\n",
    "from utils import find_closest_train_segment\n",
    "from hdst import HDST\n",
    "import torch\n",
    "import gc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14918,
     "status": "ok",
     "timestamp": 1741792084698,
     "user": {
      "displayName": "Jia Wang",
      "userId": "17769915354902194661"
     },
     "user_tz": 240
    },
    "id": "u71jt5-h33Nd",
    "outputId": "06af210d-4aab-477c-a4b1-0716d2e84bc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jKhf9ndv3xPH"
   },
   "source": [
    "# **T-Rep tutorial**\n",
    "\n",
    "The goal of this tutorial is to show you in depth:\n",
    "\n",
    "1. How to instantiate T-Rep with the most important parameters.\n",
    "2. How to train T-Rep.\n",
    "3. How to use a trained model to encode test data, at different granularities.\n",
    "\n",
    "This tutorial is more in-depth than the 'quick tutorial', as it aims to explain the parameters of the various functions used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7EQUvECQ3xPI"
   },
   "source": [
    "#### **1. Instantiate ``Args`` Configuration Class**\n",
    "\n",
    "The ``Args`` class is normally imported from the `train_eval.py` file but it is redefined here with extra comments for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 30,
     "status": "ok",
     "timestamp": 1742141034652,
     "user": {
      "displayName": "Jia Wang",
      "userId": "17769915354902194661"
     },
     "user_tz": 240
    },
    "id": "0NF-Bp7d3xPJ"
   },
   "outputs": [],
   "source": [
    "# @dataclass\n",
    "# class Args:\n",
    "#     # MODEL PARAMETERS\n",
    "#     task_weights: dict # Weights to attribute to each pretext task\n",
    "#     repr_dims: int = 128 # Latent representation dimensionality\n",
    "#     time_embedding: str = None # Time embedding to use ('t2v_sin', 'fully_learnable_big', 'gaussian', 'hybrid' etc.), 'None' for no time embedding. All implemented time-embeddings are defined in models.time_embeddings.py\n",
    "\n",
    "#     # TRAINING PARAMETERS\n",
    "#     epochs: int = 80 # Maximum number of training epochs\n",
    "#     iters: int = None # Maximum number of training iterations. Can be set to 'None' if epochs is set.\n",
    "#     batch_size: int = 16 # Training batch size\n",
    "#     lr: float = 0.001 # Learning rate\n",
    "#     seed: int = 1234 # Random seed for reproducibility\n",
    "#     max_train_length: int = 800 # Maximum sequence length (depends on your GPU memory).\n",
    "#                            # Longer sequences will be cut into smaller sequences.\n",
    "\n",
    "#     # CONFIGURATION\n",
    "#     dataset: str = \"\" # Set to \"\" if using your own dataset. If you use UCR/UEA datasets, the dataset name\n",
    "#     loader: str = \"\" # Set to \"\" if using your own dataset, otherwise \"UEA\" or \"UCR\"\n",
    "#     gpu: int = 0 # The gpu no. used for training and inference (defaults to 0)\n",
    "#     run_name: str = \"\" # Run name to save model\n",
    "#     save_every = None # Save the model checkpoint every <save_every> iterations/epochs\n",
    "#     max_threads = None # The maximum allowed number of threads used by this process. Set to None if unsure.\n",
    "#     eval: bool = True # Evaluate model after training if True (doesn't work for custom datasets, only UCR/UEA/ETT/Yahoo)\n",
    "#     irregular: float = 0.0 # Ratio of missing data (defaults to 0). Used for testing model resilience under missing data regime\n",
    "#     label_ratio: int = 1.0 # Ratio of available training labels (defaults to 1.0). Used for testing model resilience under missing labels regime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 123,
     "status": "ok",
     "timestamp": 1742158754890,
     "user": {
      "displayName": "Jia Wang",
      "userId": "17769915354902194661"
     },
     "user_tz": 240
    },
    "id": "h_2IJ1CZ6t9Y"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Args:\n",
    "    task_weights: dict\n",
    "    dataset: str = \"\"\n",
    "    loader: str = \"\"\n",
    "    gpu: int = 0\n",
    "    static_repr_dims: int = 128\n",
    "    dynamic_repr_dims: int = 128\n",
    "    epochs: int = 80\n",
    "\n",
    "    run_name: str = \"\"\n",
    "    batch_size: int = 16\n",
    "    lr: float = 0.001\n",
    "    max_train_length = 800\n",
    "    iters: int = None\n",
    "    save_every = None\n",
    "    seed: int = 1234\n",
    "    max_threads = None\n",
    "    eval: bool = True\n",
    "    irregular = 0\n",
    "\n",
    "    sample_size: int = 50\n",
    "    window_size: int = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iaJ8WBED3xPJ"
   },
   "source": [
    "- Create an instance of arguments, specifying the necessary arguments and those important to your use case.\n",
    "- Initialise device as well as config dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 286,
     "status": "ok",
     "timestamp": 1742158756687,
     "user": {
      "displayName": "Jia Wang",
      "userId": "17769915354902194661"
     },
     "user_tz": 240
    },
    "id": "XBAIx9E53xPK"
   },
   "outputs": [],
   "source": [
    "args = Args(\n",
    "    static_repr_dims=128,\n",
    "    dynamic_repr_dims=128,\n",
    "    task_weights={\n",
    "        'local_static_contrast': 1,\n",
    "        'global_vatiant_contrast': 0,\n",
    "        'dynamic_trend_pred': 0,\n",
    "    },\n",
    "    eval=False,\n",
    "    batch_size=16,\n",
    ")\n",
    "\n",
    "device = init_dl_program(args.gpu, seed=args.seed, max_threads=args.max_threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "btk0781U3xPK"
   },
   "source": [
    "#### **2. Load your data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MpQpVnqe3xPK"
   },
   "source": [
    "You can use any data, as long as it is an `np.ndarray` of shape $(N, T, C)$ where $N$ is the number of time-series instances, $T$ the number of timesteps per instance, and $C$ the number of channels.\n",
    "\n",
    "Here, we use a UCR dataset as an example, but you can use any dataset of yours.\n",
    "\n",
    "**N.B:** For the following cell to work, you will have to have downloaded the `UCR` datasets and placed them in the `datasets/UCR/` folder as instructed in the `README.md`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 85,
     "status": "ok",
     "timestamp": 1742158758652,
     "user": {
      "displayName": "Jia Wang",
      "userId": "17769915354902194661"
     },
     "user_tz": 240
    },
    "id": "zSXfH2GY3xPL",
    "outputId": "038dd67f-73c8-4663-a668-d9b96542dc70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes - train data: (1, 8640, 7), test data: (1, 2880, 7)\n"
     ]
    }
   ],
   "source": [
    "data, train_slice, valid_slice, test_slice, scaler, pred_lens = datautils.load_forecast_csv(\"ETTh1\")\n",
    "train_data = data[:, train_slice]\n",
    "test_data = data[:, test_slice]\n",
    "print(f\"Shapes - train data: {train_data.shape}, test data: {test_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LNJMAMJ93xPM"
   },
   "source": [
    "#### **3. Create and train T-Rep**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 199,
     "status": "ok",
     "timestamp": 1742158760222,
     "user": {
      "displayName": "Jia Wang",
      "userId": "17769915354902194661"
     },
     "user_tz": 240
    },
    "id": "Wsexbv4kbufc",
    "outputId": "5a9f95d0-7f6f-46bc-eb84-166023f2987a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2767"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sns.set_theme()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 50851,
     "status": "ok",
     "timestamp": 1742158832958,
     "user": {
      "displayName": "Jia Wang",
      "userId": "17769915354902194661"
     },
     "user_tz": 240
    },
    "id": "MlTPsxM23xPM",
    "outputId": "49cf31e9-eef5-4221-dc5d-8b11992ed020"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (10, 864, 7)\n",
      "Epoch #0: loss=2.6888856887817383\n",
      "Epoch #1: loss=1.8564746379852295\n",
      "Epoch #2: loss=1.670044183731079\n",
      "Epoch #3: loss=1.6827473640441895\n",
      "Epoch #4: loss=1.0348690748214722\n",
      "Epoch #5: loss=1.0289524793624878\n",
      "Epoch #6: loss=1.0608772039413452\n",
      "Epoch #7: loss=0.8572996854782104\n",
      "Epoch #8: loss=0.8317501544952393\n",
      "Epoch #9: loss=0.8209478259086609\n",
      "Epoch #10: loss=0.7570945024490356\n",
      "Epoch #11: loss=0.5682134628295898\n",
      "Epoch #12: loss=0.41528385877609253\n",
      "Epoch #13: loss=0.45433613657951355\n",
      "Epoch #14: loss=0.48441940546035767\n",
      "Epoch #15: loss=0.4396260976791382\n",
      "Epoch #16: loss=0.3055073618888855\n",
      "Epoch #17: loss=0.28080248832702637\n",
      "Epoch #18: loss=0.1545635163784027\n",
      "Epoch #19: loss=0.16639582812786102\n",
      "Epoch #20: loss=0.1722675859928131\n",
      "Epoch #21: loss=0.1796943098306656\n",
      "Epoch #22: loss=0.25509920716285706\n",
      "Epoch #23: loss=0.10068278759717941\n",
      "Epoch #24: loss=0.2009529173374176\n",
      "Epoch #25: loss=0.12905964255332947\n",
      "Epoch #26: loss=0.12954238057136536\n",
      "Epoch #27: loss=0.14839030802249908\n",
      "Epoch #28: loss=0.13741767406463623\n",
      "Epoch #29: loss=0.15255874395370483\n",
      "Epoch #30: loss=0.09856972843408585\n",
      "Epoch #31: loss=0.1176132783293724\n",
      "Epoch #32: loss=0.0992179661989212\n",
      "Epoch #33: loss=0.10618412494659424\n",
      "Epoch #34: loss=0.08062867075204849\n",
      "Epoch #35: loss=0.08928942680358887\n",
      "Epoch #36: loss=0.0496666356921196\n",
      "Epoch #37: loss=0.05954549461603165\n",
      "Epoch #38: loss=0.06291511654853821\n",
      "Epoch #39: loss=0.05954553931951523\n",
      "Epoch #40: loss=0.07728126645088196\n",
      "Epoch #41: loss=0.08167474716901779\n",
      "Epoch #42: loss=0.10672558099031448\n",
      "Epoch #43: loss=0.08187730610370636\n",
      "Epoch #44: loss=0.10869069397449493\n",
      "Epoch #45: loss=0.051905471831560135\n",
      "Epoch #46: loss=0.05297733098268509\n",
      "Epoch #47: loss=0.08961204439401627\n",
      "Epoch #48: loss=0.09358680248260498\n",
      "Epoch #49: loss=0.0775795578956604\n",
      "Epoch #50: loss=0.11608439683914185\n",
      "Epoch #51: loss=0.09490524977445602\n",
      "Epoch #52: loss=0.055209383368492126\n",
      "Epoch #53: loss=0.04409368708729744\n",
      "Epoch #54: loss=0.030024787411093712\n",
      "Epoch #55: loss=0.05449743568897247\n",
      "Epoch #56: loss=0.06859937310218811\n",
      "Epoch #57: loss=0.0666368305683136\n",
      "Epoch #58: loss=0.05976872891187668\n",
      "Epoch #59: loss=0.02419283054769039\n",
      "Epoch #60: loss=0.028969401493668556\n",
      "Epoch #61: loss=0.04811026155948639\n",
      "Epoch #62: loss=0.03793078660964966\n",
      "Epoch #63: loss=0.024896102026104927\n",
      "Epoch #64: loss=0.0174150001257658\n",
      "Epoch #65: loss=0.04564919322729111\n",
      "Epoch #66: loss=0.011776893399655819\n",
      "Epoch #67: loss=0.009570037946105003\n",
      "Epoch #68: loss=0.009148653596639633\n",
      "Epoch #69: loss=0.04271650314331055\n",
      "Epoch #70: loss=0.024755660444498062\n",
      "Epoch #71: loss=0.025393832474946976\n",
      "Epoch #72: loss=0.008697418496012688\n",
      "Epoch #73: loss=0.018968142569065094\n",
      "Epoch #74: loss=0.009609244763851166\n",
      "Epoch #75: loss=0.00641898438334465\n",
      "Epoch #76: loss=0.015489913523197174\n",
      "Epoch #77: loss=0.02029505744576454\n",
      "Epoch #78: loss=0.02124977484345436\n",
      "Epoch #79: loss=0.019435379654169083\n"
     ]
    }
   ],
   "source": [
    "model = HDST(\n",
    "    input_dims=train_data.shape[-1],\n",
    "    device=device,\n",
    "    task_weights=args.task_weights,\n",
    "    batch_size=args.batch_size,\n",
    "    lr=args.lr,\n",
    "    output_dims1=args.static_repr_dims,\n",
    "    output_dims2=args.dynamic_repr_dims,\n",
    "    max_train_length=args.max_train_length\n",
    ")\n",
    "\n",
    "loss_log = model.fit(\n",
    "    train_data,\n",
    "    n_epochs=args.epochs,\n",
    "    n_iters=args.iters,\n",
    "    k=args.sample_size,\n",
    "    w=args.window_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 448,
     "status": "ok",
     "timestamp": 1742157730649,
     "user": {
      "displayName": "Jia Wang",
      "userId": "17769915354902194661"
     },
     "user_tz": 240
    },
    "id": "kEEm1UdisE9k"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Assume model is your trained PyTorch model\n",
    "torch.save(model, 'mymodel.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1742158045054,
     "user": {
      "displayName": "Jia Wang",
      "userId": "17769915354902194661"
     },
     "user_tz": 240
    },
    "id": "IFw6i7DOsdxO",
    "outputId": "91a61e75-0b4e-47d3-8979-6397a486ac87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.6888856887817383, 1.8564746379852295, 1.670044183731079, 1.6827473640441895, 1.0348690748214722, 1.0289524793624878, 1.0608772039413452, 0.8572996854782104, 0.8317501544952393, 0.8209478259086609, 0.7570945024490356, 0.5682134628295898, 0.41528385877609253, 0.45433613657951355, 0.48441940546035767, 0.4396260976791382, 0.3055073618888855, 0.28080248832702637, 0.1545635163784027, 0.16639582812786102, 0.1722675859928131, 0.1796943098306656, 0.25509920716285706, 0.10068278759717941, 0.2009529173374176, 0.12905964255332947, 0.12954238057136536, 0.14839030802249908, 0.13741767406463623, 0.15255874395370483, 0.09856972843408585, 0.1176132783293724, 0.0992179661989212, 0.10618412494659424, 0.08062867075204849, 0.08928942680358887, 0.0496666356921196, 0.05954549461603165, 0.06291511654853821, 0.05954553931951523, 0.07728126645088196, 0.08167474716901779, 0.10672558099031448, 0.08187730610370636, 0.10869069397449493, 0.051905471831560135, 0.05297733098268509, 0.08961204439401627, 0.09358680248260498, 0.0775795578956604, 0.11608439683914185, 0.09490524977445602, 0.055209383368492126, 0.04409368708729744, 0.030024787411093712, 0.05449743568897247, 0.06859937310218811, 0.0666368305683136, 0.05976872891187668, 0.02419283054769039, 0.028969401493668556, 0.04811026155948639, 0.03793078660964966, 0.024896102026104927, 0.0174150001257658, 0.04564919322729111, 0.011776893399655819, 0.009570037946105003, 0.009148653596639633, 0.04271650314331055, 0.024755660444498062, 0.025393832474946976, 0.008697418496012688, 0.018968142569065094, 0.009609244763851166, 0.00641898438334465, 0.015489913523197174, 0.02029505744576454, 0.02124977484345436, 0.019435379654169083]\n"
     ]
    }
   ],
   "source": [
    "print(loss_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8KTanc8PYAUL"
   },
   "source": [
    "I want to test the max_cross_corr function in utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 46,
     "status": "ok",
     "timestamp": 1742154391363,
     "user": {
      "displayName": "Jia Wang",
      "userId": "17769915354902194661"
     },
     "user_tz": 240
    },
    "id": "S8xkTrHHX_gj",
    "outputId": "1464b552-ced6-45c9-faa9-4ecd32d5eb70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window1: torch.Size([50, 7])\n",
      "window2: torch.Size([50, 7])\n",
      "cc: torch.Size([7, 50])\n",
      "tensor(23.5859, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "def max_cross_corr(window1,window2):\n",
    "    \"\"\"\n",
    "    Compute the maxmium cross correlation between window1 and window2.\n",
    "    \"\"\"\n",
    "    print(\"window1:\",window1.shape)\n",
    "    print(\"window2:\",window2.shape)\n",
    "\n",
    "    L,C = window1.shape #L: length of window; C: dimension of feature\n",
    "    window1 = window1.permute(1, 0).contiguous() # C x L\n",
    "    window2 = window2.permute(1, 0).contiguous()\n",
    "    window1 = window1 - window1.mean(dim=-1, keepdim=True)\n",
    "    window2 = window2 - window2.mean(dim=-1, keepdim=True)\n",
    "\n",
    "    window1_fft = torch.fft.rfft(window1, dim=-1)\n",
    "    window2_fft = torch.fft.rfft(window2, dim=-1)\n",
    "\n",
    "    X = window1_fft * torch.conj(window2_fft)\n",
    "\n",
    "    power_norm = (window1.std(dim=-1, keepdim=True) * window2.std(dim=-1, keepdim=True)).to(X.dtype)\n",
    "    power_norm = torch.where(power_norm == 0, torch.ones_like(power_norm), power_norm)\n",
    "\n",
    "    X = X / power_norm\n",
    "\n",
    "    cc = torch.fft.irfft(X, n=L, dim=-1)\n",
    "    print(\"cc:\",cc.shape)\n",
    "    max_cc = cc.max(dim=-1).values\n",
    "\n",
    "    return max_cc\n",
    "# Load windows from the .pth file\n",
    "window1 = torch.load('window1.pth')\n",
    "window2 = torch.load('window2.pth')\n",
    "\n",
    "maxcc=max_cross_corr(window1,window2)\n",
    "corr=torch.mean(maxcc)\n",
    "print(corr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oBZg4DoE3xPM"
   },
   "source": [
    "#### **4. Encode representations with the trained model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jh3HozJt3xPN"
   },
   "source": [
    "When creating the train and test instances of a dataset, there are two methods:\n",
    "1. The most common for classification and clustering is to separate your train and test datasets by choosing different time series instances. This above means that for a dataset $X \\in \\mathbb{R}^{B \\times T \\times F}$, you define $X_{train}$ by slicing X along the instances or batch axis: `X_train = X[:n_train, :, :]`.\n",
    "2. For forecasting and anomaly detection, another way to build your train and test set is to use all instances up to timestep $T_{train}$ for training, and further timesteps for testing: `X_train = X[:, :T_train, :]`.\n",
    "\n",
    "If using the first method, please skip to section `4.1`. If using the second method, please read through section `4.0.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UaPvwzhh3xPN"
   },
   "source": [
    "##### **4.0. Using correct test-set time indices**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P4Ijp4yf3xPN"
   },
   "source": [
    "When splitting your train and test sets along the time axis, it is import to adequately label the timesteps corresponding to the test set: T-Rep uses timesteps to compute time-embeddings, so one shouldn't naively use timesteps $[T_{train}:T_{end}]$, or reindex the test set from timestep 0.\n",
    "\n",
    "As the model was trained on a previous section of the dataset $X_{train} = [x_{t_0}...x_{T_{train}}]$, with corresponding timesteps $[t_0...T_{train}]$, we will try to find the subsequence of $X_{train}$ which most closely resembles our test set (call that subsequence $[x_{t_a}:x_{t_b}]$, ranging from timesteps $t_a$ to $t_b$). When encoding our test set, we then feed T-Rep $X_{test}$ alongside that subsequence's timesteps $[t_a:t_b]$. This ensures we don't feed out-of-distribution inputs (timesteps) to the time-embedding module.\n",
    "\n",
    "Finding the closest segment to $X_{test}$ in the train data is very easily done using the `find_closest_train_segment` function, which uses a sliding window and the Euclidean distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1017,
     "status": "ok",
     "timestamp": 1742157598000,
     "user": {
      "displayName": "Jia Wang",
      "userId": "17769915354902194661"
     },
     "user_tz": 240
    },
    "id": "cuSkbmXQ3xPN",
    "outputId": "20e86585-f741-4220-9079-3fea3a90bba1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2880, 1)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closest_time_indices = find_closest_train_segment(\n",
    "    train_data,\n",
    "    test_data,\n",
    "    squared_dist=True\n",
    ")\n",
    "closest_time_indices.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tmPmGygY3xPN"
   },
   "source": [
    "##### **4.1. Encoding representations for forecasting and anomaly detection**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6yrb1GZ63xPN"
   },
   "source": [
    "Encode representations at a timestep granularity (one representation vector per timestep), preserving the original data's temporality. This is typically what you might use for **forecasting** or **anomaly detection**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_ucd96z13xPN",
    "outputId": "f3ca50e2-8e5e-41dd-fcf3-c000698634de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-grained (timestep-wise) representation shape: (1, 2880, 128)\n"
     ]
    }
   ],
   "source": [
    "test_repr_fine = model.encode(\n",
    "    data=test_data,\n",
    "    time_indices=closest_time_indices,\n",
    "    mask=None, # Used for the Anomaly Detection protocol, can be ignored\n",
    "    encoding_window=None, # Used to control the temporal granularity of the representation\n",
    "    causal=True, # Whether to use causal convolutions (for forecasting you might want this) or not.\n",
    "    sliding_length=1, # The length of sliding window. When this param is specified, a sliding inference would be applied on the time series.\n",
    "    sliding_padding=100, # Contextual data length used for inference every sliding windows. The timestamp t's representation vector is computed using the observations located in [t - sliding_padding, t].\n",
    "    batch_size=16,\n",
    "    return_time_embeddings=False\n",
    ")\n",
    "print(f\"Fine-grained (timestep-wise) representation shape: {test_repr_fine.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LGmZV3sA3xPO"
   },
   "source": [
    "##### **4.2. Encoding representations for classification and clustering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HxZRhTH83xPO"
   },
   "source": [
    "Encode representations at an instance granularity (one representation vector per time-series instance), eliminating the temporal dimension of the data. This is more typically used for **classification** or **clustering**. For these tasks, we often discard the temporal dimension as we care more about **inter-instance** differences than **intra-instance** differences. In most cases, reducing each instance to one representation vector is enough, and helps reduce the intrinsic dimensionality of our problem.\n",
    "\n",
    "To encode representations at an instance granularity, simply set `encoding_window='full_series'`, which will apply a maxpool operation to the temporal dimension of the representation with a kernel size equal to the length of the time series, resulting in a temporal dimension of 1. You thus obtain one representation vector for entire time series instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0qXLYiiI3xPO",
    "outputId": "f71347d8-1705-4029-90fc-e14a201eb364"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instance-wide representation shape: (1, 128)\n"
     ]
    }
   ],
   "source": [
    "test_repr_coarse = model.encode(\n",
    "    data=test_data,\n",
    "    time_indices=closest_time_indices,\n",
    "    mask=None, # Used for the Anomaly Detection protocol, can be ignored\n",
    "    encoding_window='full_series', # Used to control the temporal granularity of the representation\n",
    "    causal=False, # Whether to use causal convolutions (for forecasting for instance) or not.\n",
    "    sliding_length=None, # The length of sliding window. When this param is specified, a sliding inference would be applied on the time series.\n",
    "    sliding_padding=0, # Contextual data length used for inference every sliding windows.\n",
    "    batch_size=16,\n",
    "    return_time_embeddings=False\n",
    ")\n",
    "print(f\"Instance-wide representation shape: {test_repr_coarse.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tp1XCYJc3xPO"
   },
   "source": [
    "##### **4.3. Encoding representations with custom temporal granularity**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1EYSK8Zo3xPO"
   },
   "source": [
    "Encode representations at a custom temporal granularity, by setting the ``encoding_window`` parameter to an integer. This integer specifies the kernel size that will be used to apply a **maxpool** operation to the timestep-level representation. This may be desirable for more advanced use cases in classification, clustering, anomaly detection, forecasting, or any other downstream tasks.\n",
    "\n",
    "This works by first computing the representation at full temporal granularity, i.e. with the same number of timesteps as the original data. A maxpool operation is then applied to the temporal dimension of the representation, with kernel size controled by the `encoding_window` parameter of the encoding function. The `stride` and `padding` are both set to `encoding_window // 2`. The representation's temporal dimensionality can be pre-determiend using the usual maxpool dimension formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lagQMf5U3xPO",
    "outputId": "6eaeac32-6f36-45f4-d504-c1c8b76d2e5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom temporal resolution representation shape: (1, 115, 128)\n"
     ]
    }
   ],
   "source": [
    "test_repr_custom = model.encode(\n",
    "    data=test_data,\n",
    "    time_indices=closest_time_indices,\n",
    "    mask=None, # Used for the Anomaly Detection protocol, can be ignored\n",
    "    encoding_window=50, # Used to control the temporal granularity of the representation\n",
    "    causal=False, # Whether to use causal convolutions (for forecasting for instance) or not.\n",
    "    sliding_length=None, # The length of sliding window. When this param is specified, a sliding inference would be applied on the time series.\n",
    "    sliding_padding=0, # Contextual data length used for inference every sliding windows.\n",
    "    batch_size=16,\n",
    "    return_time_embeddings=False\n",
    ")\n",
    "print(f\"Custom temporal resolution representation shape: {test_repr_custom.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QpOwqFSP3xPO"
   },
   "source": [
    "A special value can be passed to the ``encoding_window`` parameter of the encoding function: it can be set to 'multiscale'. This will concatenate representations at multiple temporal granularities, resulting in a representation which incorporates both global and local information at every timestep. Essentially this results in a larger representation dimensionality, but no temporal resolution change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sNPUgmPJ3xPP",
    "outputId": "65e96d4a-50f5-4494-9e70-39519e368b36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiscale representation shape: (1, 2880, 1024)\n"
     ]
    }
   ],
   "source": [
    "test_repr_multiscale = model.encode(\n",
    "    data=test_data,\n",
    "    time_indices=closest_time_indices,\n",
    "    mask=None, # Used for the Anomaly Detection protocol, can be ignored\n",
    "    encoding_window='multiscale', # Used to control the temporal granularity of the representation\n",
    "    causal=False, # Whether to use causal convolutions (for forecasting for instance) or not.\n",
    "    sliding_length=1, # The length of sliding window. When this param is specified, a sliding inference would be applied on the time series.\n",
    "    sliding_padding=100, # Contextual data length used for inference every sliding windows.\n",
    "    batch_size=16,\n",
    "    return_time_embeddings=False\n",
    ")\n",
    "print(f\"Multiscale representation shape: {test_repr_multiscale.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MSMk3K0i3xPP"
   },
   "source": [
    "#### **That's it!**\n",
    "\n",
    "This is all you need to know to use T-Rep. The produced `np.ndarray` of representations can then be used as inputs for any task ranging from classification, clustering, forecasting, to anomaly detection etc."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
