{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def extract_fixed_random_windows(x, w, k):\n",
    "    \"\"\"\n",
    "    Extracts k random windows of size w using the same k starting points for all batch samples.\n",
    "\n",
    "    Args:\n",
    "        x (torch.Tensor): Input time-series tensor of shape (batch_size, n_timestamps, n_features).\n",
    "        w (int): Window size.\n",
    "        k (int): Number of windows per sample.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Extracted windows of shape (batch_size, k, w, n_features).\n",
    "    \"\"\"\n",
    "    batch_size, n_timestamps, n_features = x.shape\n",
    "    \n",
    "    # Generate k random start indices (same for all samples)\n",
    "    crop_start = np.random.choice(range(0, n_timestamps - w), size=k, replace=False)   # Shape: (k,)\n",
    "    print(\"crop start\")\n",
    "    print(crop_start)\n",
    "\n",
    "    # Create index tensors\n",
    "    idx = torch.arange(w).repeat(k, 1)  # Shape: (k, w)\n",
    "    print(idx)\n",
    "    crop_start_tensor = torch.tensor(crop_start).unsqueeze(-1)  # Shape: (k, 1)\n",
    "    print(\"crop start\")\n",
    "    print(crop_start_tensor)\n",
    "\n",
    "    # Compute the actual indices\n",
    "    indices = crop_start_tensor + idx  # Shape: (k, w)\n",
    "    print(\"indices\")\n",
    "    print(indices)\n",
    "\n",
    "    # Expand indices to match batch size\n",
    "    indices = indices.unsqueeze(0).expand(batch_size, -1, -1)  # Shape: (batch_size, k, w)\n",
    "    print(\"indices\")\n",
    "    print(indices)\n",
    "\n",
    "    # Use advanced indexing to extract the windows\n",
    "    batch_idx = torch.arange(batch_size).view(batch_size, 1, 1).expand(-1, k, w)\n",
    "    print(batch_idx)\n",
    "\n",
    "    extracted_windows = x[batch_idx, indices]  # Shape: (batch_size, k, w, n_features)\n",
    "\n",
    "    return extracted_windows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crop start\n",
      "[ 1 13]\n",
      "tensor([[0, 1, 2, 3],\n",
      "        [0, 1, 2, 3]])\n",
      "crop start\n",
      "tensor([[ 1],\n",
      "        [13]])\n",
      "indices\n",
      "tensor([[ 1,  2,  3,  4],\n",
      "        [13, 14, 15, 16]])\n",
      "indices\n",
      "tensor([[[ 1,  2,  3,  4],\n",
      "         [13, 14, 15, 16]],\n",
      "\n",
      "        [[ 1,  2,  3,  4],\n",
      "         [13, 14, 15, 16]],\n",
      "\n",
      "        [[ 1,  2,  3,  4],\n",
      "         [13, 14, 15, 16]]])\n",
      "tensor([[[0, 0, 0, 0],\n",
      "         [0, 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1, 1],\n",
      "         [1, 1, 1, 1]],\n",
      "\n",
      "        [[2, 2, 2, 2],\n",
      "         [2, 2, 2, 2]]])\n",
      "windows\n",
      "torch.Size([3, 2, 4, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "batch_size = 3\n",
    "n_timestamps = 20\n",
    "n_features = 5\n",
    "w = 4  # Window size\n",
    "k = 2  # Number of windows per sample\n",
    "\n",
    "# Create a random tensor\n",
    "x = torch.randn(batch_size, n_timestamps, n_features)\n",
    "\n",
    "# print(\"x\")\n",
    "# print(x)\n",
    "\n",
    "# Extract k windows per sample (same across batch)\n",
    "windows = extract_fixed_random_windows(x, w, k)\n",
    "print(\"windows\")\n",
    "print(windows.shape)  # Expected output: (batch_size, k, w, n_features)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
